{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Classifier\n",
    "\n",
    "Use Yujia's BERT classifier code with all the new data we got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Packages-and-Load-Data\" data-toc-modified-id=\"Import-Packages-and-Load-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import Packages and Load Data</a></span><ul class=\"toc-item\"><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Load Data</a></span></li><li><span><a href=\"#Prepare-GPU\" data-toc-modified-id=\"Prepare-GPU-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Prepare GPU</a></span></li></ul></li><li><span><a href=\"#Data-Pre-Processing\" data-toc-modified-id=\"Data-Pre-Processing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Pre-Processing</a></span><ul class=\"toc-item\"><li><span><a href=\"#BERT-Tokenizer-and-Padding\" data-toc-modified-id=\"BERT-Tokenizer-and-Padding-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>BERT Tokenizer and Padding</a></span></li><li><span><a href=\"#Attention-Masks\" data-toc-modified-id=\"Attention-Masks-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Attention Masks</a></span></li></ul></li><li><span><a href=\"#Prepare-Model\" data-toc-modified-id=\"Prepare-Model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Prepare Model</a></span><ul class=\"toc-item\"><li><span><a href=\"#Train-Test-Validation-Split\" data-toc-modified-id=\"Train-Test-Validation-Split-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Train Test Validation Split</a></span></li><li><span><a href=\"#Multilabel-Classifier\" data-toc-modified-id=\"Multilabel-Classifier-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Multilabel Classifier</a></span></li></ul></li><li><span><a href=\"#Run-Model\" data-toc-modified-id=\"Run-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Run Model</a></span></li><li><span><a href=\"#Save-Trained-Models\" data-toc-modified-id=\"Save-Trained-Models-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Save Trained Models</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "gpu_id=1\n",
    "epochs = 8\n",
    "# topics = topic_to_id.keys()\n",
    "topics = ['child product', 'pregnancy', 'dad parenting', 'multiple children', 'mom health', 'non-biological parents', 'child appearances']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(1)\n",
    "# if device.type == 'cuda':\n",
    "#     print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "#     print('Memory Usage:')\n",
    "#     print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "#     print('Cached:   ', round(torch.cuda.memory_cached(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Load Data\n",
    "\n",
    "Can be later changed to pull data from GitHub, but for now just do it from a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['20200422-multilabel.h5', '.ipynb_checkpoints', 'facebook', '0527_reddit_1300_parenting_clean.csv', 'extra_data', 'labeled_only-reddit_796_of_1300.h5', '20200405-topic_to_id.pickle', '20200405-topic_per_row.h5']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Input data files are available in the \"../data/\" directory.\n",
    "import os\n",
    "print(os.listdir(\"../data\"))\n",
    "\n",
    "# Basics + Viz\n",
    "import torch\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Pre-processing\n",
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Models\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils import flat_accuracy, format_time, single_topic_train, augmented_validationloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259, 33)\n",
      "(665, 33)\n",
      "(59, 33)\n",
      "(590, 33)\n",
      "(796, 33)\n",
      "(249, 33)\n",
      "Total: (2618, 33)\n"
     ]
    }
   ],
   "source": [
    "# load dictionary\n",
    "with open(\"../data/20200405-topic_to_id.pickle\", \"rb\") as input_file:\n",
    "    topic_to_id = pickle.load(input_file)\n",
    "\n",
    "# load data\n",
    "data_folder = '../data/extra_data/aug/'\n",
    "df = pd.DataFrame()\n",
    "file_names = os.listdir(data_folder)\n",
    "\n",
    "for f in file_names:\n",
    "    temp = pd.read_csv(data_folder + f)\n",
    "    print(temp.shape)\n",
    "    df = pd.concat([df, temp])\n",
    "\n",
    "print(f\"Total: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "4V7iUEJbJgdU",
    "outputId": "4ac6fb0f-4859-4c7f-e2b0-bc63ad439876"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(gpu_id)\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing\n",
    "\n",
    "Use https://github.com/huggingface/transformers BERTTokenizer to change all the words into IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Tokenizer and Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "PavXKE1PJ26F",
    "outputId": "d6846f77-67c3-4e60-aaed-0cd75497fcbc",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n"
     ]
    }
   ],
   "source": [
    "# Load the BERT tokenizer.\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (601 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (580 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (835 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (662 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (766 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (666 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (566 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (522 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (578 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (791 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1033 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (962 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (652 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (569 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (698 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1677 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1890 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (898 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (845 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (511 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (551 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (553 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (555 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (887 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1602 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (586 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (579 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (513 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (638 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (987 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (682 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (707 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (831 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (970 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (798 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (799 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (596 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (623 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1082 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (567 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (562 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (552 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (542 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (668 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1923 > 512). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (585 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence length.\n",
    "MAX_LEN = 512\n",
    "\n",
    "sentence_lengths = []\n",
    "def tokenize_and_count(s, lst, max_len):\n",
    "    # `encode` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    answer = tokenizer.encode(s, add_special_tokens=True)\n",
    "    lst.append(len(answer))\n",
    "    \n",
    "    return answer\n",
    "\n",
    "df['bert'] = df.text.apply(lambda s : tokenize_and_count(s, sentence_lengths, MAX_LEN))\n",
    "df['bert_aug'] = df.aug.apply(lambda s : tokenize_and_count(s, sentence_lengths, MAX_LEN))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's obvious the default MAX_LEN=512 is not enough for some posts, but just how long are these posts?<br>\n",
    "<br>\n",
    "Turns out only 2% of all the sentences are above the length 512.<br>\n",
    "So we'll just proceed as normal and truncate/extend all the sentences to length 512, as most sentences are distributed between the 100~200 word range, we don't want to add too many padding to most sentences by setting the MAX_LEN to something too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of the\n",
      "5236 total sentences,\n",
      "55 are over the length 512,\n",
      "Total of: 1.05%\n"
     ]
    }
   ],
   "source": [
    "max_len = 512\n",
    "temp = np.array(sentence_lengths)\n",
    "temp_count = len(temp[temp > max_len])\n",
    "temp_len = len(sentence_lengths)\n",
    "\n",
    "print(f\"Out of the\\n{temp_len} total sentences,\\n{temp_count} are over the length {max_len},\\nTotal of: {(temp_count/temp_len * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "n, bins, patches = plt.hist(sentence_lengths, bins=[30 * i for i in range(50)], facecolor='red', alpha=0.5)\n",
    "_ = plt.axvline(512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad our input tokens with value 0.\n",
    "# \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "# as opposed to the beginning.\n",
    "df['bert'] = pad_sequences(df['bert'].values, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\").tolist()\n",
    "df['bert_aug'] = pad_sequences(df['bert_aug'].values, maxlen=MAX_LEN, dtype=\"long\", \n",
    "                          value=0, truncating=\"post\", padding=\"post\").tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Masks\n",
    "\n",
    "Source: https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "<br>\n",
    "Attention masks are used to filter out the padding from each sentence. A simple format of 1 for a real word and 0 for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create attention masks\n",
    "df['attention'] = df['bert'].apply(lambda arr : [int(token_id > 0) for token_id in arr])\n",
    "df['attention_aug'] = df['bert_aug'].apply(lambda arr : [int(token_id > 0) for token_id in arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8/0.1/0.1 split\n",
      "2094 lines of training data,\n",
      "262 lines of test data\n",
      "262 lines of validation data\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.2\n",
    "validation_size = 0.5\n",
    "train_df, test_df = train_test_split(df, random_state=42, test_size=test_size)\n",
    "test_df, validation_df = train_test_split(test_df, random_state=42, test_size=validation_size)\n",
    "\n",
    "print(f\"\"\"{1 - test_size}/{test_size * (1-validation_size)}/{test_size * validation_size} split\n",
    "{train_df.shape[0]} lines of training data,\n",
    "{test_df.shape[0]} lines of test data\n",
    "{validation_df.shape[0]} lines of validation data\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilabel Classifier\n",
    "__[TODO]__ Turns out this is harder to do. Figure this out later.\n",
    "\n",
    "For this classifier we are going to throw in all 30 labels as one big multilabel.<br>\n",
    "This is happening first mostly because it's easier to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run all the training for the topics we want to run.<br>\n",
    "Later, when we are sure of the model we're to use, we'll be running it for all 30 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 5e-05: [0.08048289738430583, 0.20547935773499684, 0.3216135952708843, 0.5200226244343892, 0.5200214822771213, 0.5113378684807256, 0.5672900199215989, 0.5602143887858173]\n",
      "lr = 3e-05: [0.07589285714285714, 0.2802099835367937, 0.5239240140500645, 0.6545634643638273, 0.6899132446500869, 0.6846849245830571, 0.6862301217335143, 0.6884976954296594]\n",
      "lr = 2e-05: [0.10860484544695072, 0.42453606151085144, 0.5594092182327477, 0.7194470278229869, 0.7149276383099796, 0.7079735740450027, 0.7281931836279661, 0.7142474809141477]\n"
     ]
    }
   ],
   "source": [
    "lrs = [5e-5, 3e-5, 2e-5]\n",
    "\n",
    "for lr in lrs:\n",
    "    avg_f1s = np.zeros(epochs, dtype=float)\n",
    "\n",
    "    # Create x, y for each\n",
    "    for topic in topics[::-1]:\n",
    "        train_dataloader, test_dataloader, validation_dataloader = augmented_validationloader(train_df,\n",
    "                                                                                              test_df,\n",
    "                                                                                              validation_df,\n",
    "                                                                                              topic,\n",
    "                                                                                              batch_size)\n",
    "\n",
    "        # Then load the pretrained BERT model (has linear classification layer on top)\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        model.cuda(device=device)\n",
    "\n",
    "        # load optimizer\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr, # args.learning_rate - default is 5e-5\n",
    "                      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                    )\n",
    "\n",
    "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0,\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "        arg_dict = {\n",
    "            \"device\" : device,\n",
    "            \"optimizer\" : optimizer,\n",
    "            \"scheduler\" : scheduler,\n",
    "            \"model\" : model,\n",
    "            \"epochs\" : epochs,\n",
    "            \"train_dataloader\" : train_dataloader,\n",
    "            \"test_dataloader\" : validation_dataloader,\n",
    "            \"seed_val\" : 42,\n",
    "            \"get_f1s\" : True,\n",
    "            \"verbose\" : False\n",
    "        }\n",
    "        model, train_losses, test_losses, f1s = single_topic_train(**arg_dict)\n",
    "        avg_f1s += np.array(f1s)\n",
    "    print(f\"lr = {lr}: {(avg_f1s / len(topics)).tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 1e-05: [0.06516290726817042, 0.24675324675324675, 0.5842891002554867, 0.6294093705858411, 0.6860166288737718, 0.7122598430869107, 0.7346648060933775, 0.7492931353690847]\n",
      "lr = 5e-06: [0.06896551724137931, 0.15912087912087913, 0.24770044770044766, 0.4942900237017884, 0.5530499848910312, 0.5631834281540739, 0.6011988011988011, 0.6061115355233001]\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-5, 5e-6]\n",
    "\n",
    "for lr in lrs:\n",
    "    avg_f1s = np.zeros(epochs, dtype=float)\n",
    "\n",
    "    # Create x, y for each\n",
    "    for topic in topics[::-1]:\n",
    "        train_dataloader, test_dataloader, validation_dataloader = augmented_validationloader(train_df,\n",
    "                                                                                              test_df,\n",
    "                                                                                              validation_df,\n",
    "                                                                                              topic,\n",
    "                                                                                              batch_size)\n",
    "\n",
    "        # Then load the pretrained BERT model (has linear classification layer on top)\n",
    "        model = BertForSequenceClassification.from_pretrained(\n",
    "            \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "            num_labels = 2, # The number of output labels--2 for binary classification.\n",
    "                            # You can increase this for multi-class tasks.   \n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "\n",
    "        model.cuda(device=device)\n",
    "\n",
    "        # load optimizer\n",
    "        optimizer = AdamW(model.parameters(),\n",
    "                      lr = lr, # args.learning_rate - default is 5e-5\n",
    "                      eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                    )\n",
    "\n",
    "        # Total number of training steps is [number of batches] x [number of epochs]. \n",
    "        total_steps = len(train_dataloader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                    num_warmup_steps = 0,\n",
    "                                                    num_training_steps = total_steps)\n",
    "\n",
    "        arg_dict = {\n",
    "            \"device\" : device,\n",
    "            \"optimizer\" : optimizer,\n",
    "            \"scheduler\" : scheduler,\n",
    "            \"model\" : model,\n",
    "            \"epochs\" : epochs,\n",
    "            \"train_dataloader\" : train_dataloader,\n",
    "            \"test_dataloader\" : validation_dataloader,\n",
    "            \"seed_val\" : 42,\n",
    "            \"get_f1s\" : True,\n",
    "            \"verbose\" : False\n",
    "        }\n",
    "        model, train_losses, test_losses, f1s = single_topic_train(**arg_dict)\n",
    "        avg_f1s += np.array(f1s)\n",
    "    print(f\"lr = {lr}: {(avg_f1s / len(topics)).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "daUS3De4VV3J"
   },
   "outputs": [],
   "source": [
    "# # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
    "\n",
    "# output_dir = './{}_model_save/'.format(\"child_product\".replace(' ', '_'))\n",
    "# print(output_dir)\n",
    "# # Create output directory if needed\n",
    "# if not os.path.exists(output_dir):\n",
    "#     os.makedirs(output_dir)\n",
    "\n",
    "# print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# # They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "# model_to_save.save_pretrained(output_dir)\n",
    "# tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "# # Good practice: save your training arguments together with the trained model\n",
    "# # torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PARENTING_BERT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "182px",
    "width": "182px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1275.02px",
    "left": "45px",
    "top": "111.483px",
    "width": "265.083px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
